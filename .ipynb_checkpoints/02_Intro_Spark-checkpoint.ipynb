{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=15%><img src=\"./img/UGA.png\"></img></td>\n",
    "<td><center><h1>Convex and Distributed Optimization</h1></center></td>\n",
    "<td width=15%><a href=\"http://www.iutzeler.org\" style=\"font-size: 16px; font-weight: bold\">Franck Iutzeler</a><br/>\n",
    "<a href=\"https://ljk.imag.fr/membres/Jerome.Malick/\" style=\"font-size: 16px; font-weight: bold\">Jérôme Malick</a><br/>\n",
    "<a href=\"https://tropars.github.io/\" style=\"font-size: 16px; font-weight: bold\">Thomas Ropars</a><br/>2017/2018 </td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><div id=\"top\"></div>\n",
    "\n",
    "<center><a style=\"font-size: 40pt; font-weight: bold\">Lab. 2 - Introduction to Spark </a></center>\n",
    "\n",
    "<br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Python version] \t3.6.2 |Anaconda custom (64-bit)| (default, Sep 19 2017, 08:03:39) [MSC v.1900 64 bit (AMD64)]\n",
      "\n",
      "[Packages versions]\n",
      "\n",
      "IPython             :\t6.1.0\n",
      "numpy               :\t1.12.1\n",
      "matplotlib          :\t2.0.2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "div.warn {    \n",
       "    background-color: #fcf2f2;\n",
       "    border-color: #dFb5b4;\n",
       "    border-left: 5px solid #dfb5b4;\n",
       "    padding: 0.5em;\n",
       "    }\n",
       "\n",
       "\n",
       "div.exo {    \n",
       "    background-color: #e6ffe6;\n",
       "    border-color: #006400;\n",
       "    border-left: 5px solid #006400;\n",
       "    padding: 0.5em;\n",
       "    }\n",
       "\n",
       "\n",
       "\n",
       "tt {    \n",
       "    background-color: #e7e7e7;\n",
       "    font-size: 13px;\n",
       "    }\n",
       " </style>\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lib.notebook_setting as nbs\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "packageList = ['IPython', 'numpy', 'matplotlib']\n",
    "nbs.packageCheck(packageList)\n",
    "\n",
    "nbs.cssStyling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Outline</b><br/><br/>\n",
    "&nbsp;&nbsp;&nbsp; 1) <a href=\"#Intro\"> Introduction </a><br/>\n",
    "&nbsp;&nbsp;&nbsp; 2) <a href=\"#ML\"> MovieLens Dataset</a><br/>\n",
    "&nbsp;&nbsp;&nbsp; 3) <a href=\"#Trans\"> Parsing and Transformations with Spark </a><br/>\n",
    "&nbsp;&nbsp;&nbsp; 4) <a href=\"#LibSVM\"> LibSVM Format </a><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"Intro\"> 1) Introduction</a>  \n",
    "\n",
    "\n",
    "<p style=\"text-align: right; font-size: 10px;\"><a href=\"#top\">Go to top</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these hands-on exercices we will be focusing on manipulating Resilient Distributed Datasets (RDDs). We introduce `map`, `mapValues`, `reduce`, `reduceByKey`, `aggregateByKey`, `filter` and `join` to transform, aggregate, and connect datasets. Each function can be stringed together to do more complex tasks.\n",
    "\n",
    "The first part deals with movieLens dataset. These datasets will be used to build a movie' recommendation system based on Non Negative Matrix Factorization (NMF) methodology (Part II). In this part we work together as __Q & A__ (Questions and Answers).\n",
    "\n",
    "The second part (data processing of textual dataset) is your home work to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local appName=test>\n"
     ]
    }
   ],
   "source": [
    "# set up spark environment (Using Spark Local Mode)\n",
    "import os\n",
    "import sys\n",
    "\n",
    "spark_path = \"E:\\Spark\"\n",
    "\n",
    "os.environ['SPARK_HOME'] = spark_path\n",
    "os.environ['HADOOP_HOME'] = spark_path\n",
    "\n",
    "sys.path.append(spark_path + \"/bin\")\n",
    "sys.path.append(spark_path + \"/python\")\n",
    "sys.path.append(spark_path + \"/python/pyspark/\")\n",
    "sys.path.append(spark_path + \"/python/lib\")\n",
    "sys.path.append(spark_path + \"/python/lib/pyspark.zip\")\n",
    "sys.path.append(spark_path + \"/python/lib/py4j-0.9-src.zip\")\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "sc = SparkContext(\"local\", \"test\")\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.30.1.14:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=test>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every `SparkContext` launches a web UI, that displays useful information about the application. \n",
    "\n",
    "- A list of scheduler stages and tasks\n",
    "- A summary of RDD sizes and memory usage\n",
    "- Environmental information\n",
    "- Information about the running executors\n",
    "\n",
    "We can access this interface by looking in the terminal for a line\n",
    "\n",
    "    INFO SparkUI: Started SparkUI at http://172.17.0.1:4040\n",
    "\n",
    "and opening the URL (http://172.17.0.1:4040 in this case) in a web browser.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"ML\"> 2) MovieLens Dataset</a>  \n",
    "\n",
    "\n",
    "<p style=\"text-align: right; font-size: 10px;\"><a href=\"#top\">Go to top</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with ratings from users on movies, collected by [MovieLens](https://movielens.org). This dataset is pre-loaded under `data/movielens/`. For quick testing of your code, you may want to use a smaller dataset under `data/movielens/medium`, which contains 1 million ratings from 6000 users on 4000 movies.\n",
    "\n",
    "We will use two files from this dataset: `ratings.dat` and `movies.dat`. All ratings are contained in the file `ratings.dat` and are in the following format:\n",
    "\n",
    "```\n",
    "UserID::MovieID::Rating::Timestamp\n",
    "```\n",
    "The movie information is in the file `movies.dat` and is in the following format:\n",
    "\n",
    "```\n",
    "MovieID::Title::Genres\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's start with the data. Loading the dataset:\n",
    "- [MovieLens 1M Dataset](http://grouplens.org/datasets/movielens/1m/ml-1m.zip) - 1 million ratings from 6000 users on 4000 movies.\n",
    "- [MovieLens 20M Dataset](http://grouplens.org/datasets/movielens/20m/ml-20m.zip) - 20 million ratings and 465,000 tag applications applied to 27,000 movies by 138,000 users. \n",
    "- [MovieLens latest Dataset](http://grouplens.org/datasets/movielens/20m/ml-20m.zip) - 22 million ratings and 580,000 tag applications applied to 33,000 movies by 240,000 users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Example:</b> Below, we define two functions <tt>parseRating</tt> and <tt>parseMovie</tt> that parse a rating and a movie record.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parseRating(line):\n",
    "    \"\"\" Parse a rating record in MovieLens format UserID::MovieID::Rating::Timestamp\n",
    "    Args:\n",
    "        line (str): a line in the ratings dataset in the form of UserID::MovieID::Rating::Timestamp\n",
    "    Returns:\n",
    "        tuple: (UserID, MovieID, Rating)\n",
    "    \"\"\"\n",
    "    fields = line.split('::')\n",
    "    return int(fields[0]), int(fields[1]), float(fields[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parseMovie(line):\n",
    "    \"\"\" Parse a movie record in MovieLens format MovieID::Title::Genres\n",
    "    Args:\n",
    "        entry (str): a line in the movies dataset in the form of MovieID::Title::Genres\n",
    "    Returns:\n",
    "        tuple: (MovieID, Title, Genres)\n",
    "    \"\"\"\n",
    "    fields = line.split(\"::\")\n",
    "    return int(fields[0]), fields[1], fields[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question:</b> Create two RDDs by \n",
    "<ul>\n",
    "<li> reading a file with <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=textfile#pyspark.SparkContext.textFile\">`textFile`</a></li>\n",
    "<li> using the <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.map\">`map`</a> transformation operation with the above defined functions to create them</li>\n",
    "<li> assigning them a name with <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=setname#pyspark.RDD.setName\">`setName`</a> (e.g. `movies` and `ratings` respectively).</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path to MovieLens dataset\n",
    "movieLensHomeDir=\"data/movielens/medium/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'Toy Story (1995)', \"Animation|Children's|Comedy\"),\n",
       " (2, 'Jumanji (1995)', \"Adventure|Children's|Fantasy\"),\n",
       " (3, 'Grumpier Old Men (1995)', 'Comedy|Romance'),\n",
       " (4, 'Waiting to Exhale (1995)', 'Comedy|Drama'),\n",
       " (5, 'Father of the Bride Part II (1995)', 'Comedy')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# movies is an RDD of (movieID, title, genre)\n",
    "moviesRDD = sc.textFile(movieLensHomeDir+\"movies.dat\")\n",
    "moviesRDD.take(5)\n",
    "moviesRDD = moviesRDD.map(lambda x: parseMovie(x))\n",
    "moviesRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1193, 5.0),\n",
       " (1, 661, 3.0),\n",
       " (1, 914, 3.0),\n",
       " (1, 3408, 4.0),\n",
       " (1, 2355, 5.0),\n",
       " (1, 1197, 3.0),\n",
       " (1, 1287, 5.0),\n",
       " (1, 2804, 5.0),\n",
       " (1, 594, 4.0),\n",
       " (1, 919, 4.0)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ratings is an RDD of (userID, movieID, rating)\n",
    "ratingsRDD = sc.textFile(movieLensHomeDir+\"ratings.dat\").map(lambda x: parseRating(x))\n",
    "ratingsRDD.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__ - In these lines of code, we are creating the `moviesRDD` and `ratingsRDD` variables (technically RDDs) and we are pointing to files (on your local PC). Spark’s lazy nature means that it doesn’t automatically compile your code. Instead, it waits for some sort of action occurs that requires some calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"Trans\"> 3) Parsing and Transformations with Spark</a>  \n",
    "\n",
    "\n",
    "<p style=\"text-align: right; font-size: 10px;\"><a href=\"#top\">Go to top</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question:</b> Make your first transformation to get the number of ratings, distinct users and movies from the ratings RDD. (see the various native operations on <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=count#pyspark.RDD\">RDDs</a> in the doc) <br/>\n",
    "Display several elements of each created RDDs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000209\n",
      "6040\n",
      "3706\n"
     ]
    }
   ],
   "source": [
    "numRatings = ratingsRDD.count()\n",
    "numUsers   = ratingsRDD.map(lambda x: x[0]).distinct().count()\n",
    "numMovies  = ratingsRDD.map(lambda x: x[1]).distinct().count()\n",
    "print(numRatings)\n",
    "print(numUsers)\n",
    "print(numMovies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question:</b> Define two new RDDs containing only the movies for genre _Comedy_ and all movies that have _Comedy_ among other genres.<br/>\n",
    "Use the <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.filter\">`filter`</a> function which return a new RDD containing only the elements that satisfy a predicate.<br/>\n",
    "Use the <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.subtract\">`subtract`</a> function to retreive the movies that have  _Comedy_ in their genres but not only (That is the elements of the second RDD minus the ones in the first). Count them and exhibit a few of them.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 'Father of the Bride Part II (1995)', 'Comedy'),\n",
       " (19, 'Ace Ventura: When Nature Calls (1995)', 'Comedy'),\n",
       " (38, 'It Takes Two (1995)', 'Comedy'),\n",
       " (52, 'Mighty Aphrodite (1995)', 'Comedy'),\n",
       " (63,\n",
       "  \"Don't Be a Menace to South Central While Drinking Your Juice in the Hood (1996)\",\n",
       "  'Comedy')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comediesRDD=moviesRDD.filter(lambda x: x[2] == \"Comedy\")\n",
    "comediesRDD.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'Toy Story (1995)', \"Animation|Children's|Comedy\"),\n",
       " (3, 'Grumpier Old Men (1995)', 'Comedy|Romance'),\n",
       " (4, 'Waiting to Exhale (1995)', 'Comedy|Drama'),\n",
       " (7, 'Sabrina (1995)', 'Comedy|Romance'),\n",
       " (11, 'American President, The (1995)', 'Comedy|Drama|Romance'),\n",
       " (12, 'Dracula: Dead and Loving It (1995)', 'Comedy|Horror'),\n",
       " (21, 'Get Shorty (1995)', 'Action|Comedy|Drama'),\n",
       " (34, 'Babe (1995)', \"Children's|Comedy|Drama\"),\n",
       " (39, 'Clueless (1995)', 'Comedy|Romance'),\n",
       " (45, 'To Die For (1995)', 'Comedy|Drama'),\n",
       " (54, 'Big Green, The (1995)', \"Children's|Comedy\"),\n",
       " (64, 'Two if by Sea (1996)', 'Comedy|Romance'),\n",
       " (68, 'French Twist (Gazon maudit) (1995)', 'Comedy|Romance'),\n",
       " (70, 'From Dusk Till Dawn (1996)', 'Action|Comedy|Crime|Horror|Thriller'),\n",
       " (72, 'Kicking and Screaming (1995)', 'Comedy|Drama'),\n",
       " (75, 'Big Bully (1996)', 'Comedy|Drama'),\n",
       " (84, 'Last Summer in the Hamptons (1995)', 'Comedy|Drama'),\n",
       " (87, 'Dunston Checks In (1996)', \"Children's|Comedy\"),\n",
       " (93, 'Vampire in Brooklyn (1995)', 'Comedy|Romance'),\n",
       " (106, 'Nobody Loves Me (Keiner liebt mich) (1994)', 'Comedy|Drama')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comediesandmoreRDD=moviesRDD.filter(lambda x: \"Comedy\" in x[2] ).subtract(comediesRDD)\n",
    "comediesandmoreRDD=comediesandmoreRDD.sortBy(lambda x: x[0])\n",
    "comediesandmoreRDD.take(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question:</b> Investigate the different movies genres. Warning: Multiples genres should not be seen as new genres! For this:\n",
    "<ul>\n",
    "<li> separate the genres by delimiter '|' using  <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.flatMap\">`flatMap`</a></li>\n",
    "<li> use the <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.distinct\">`distinct`</a> function which return a new RDD containing the distinct elements in this RDD.</li>\n",
    "</ul>\n",
    "Count the number of different genres and print them.\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Genres:  18 List of Genres: [\"Children's\", 'Fantasy', 'Romance', 'Drama', 'Action', 'Thriller', 'Horror', 'Sci-Fi', 'Documentary', 'Musical', 'Western', 'Animation', 'Comedy', 'Adventure', 'Crime', 'War', 'Mystery', 'Film-Noir']\n"
     ]
    }
   ],
   "source": [
    "genres= moviesRDD.flatMap(lambda x: x[2].split(\"|\")).distinct()\n",
    "genreCount= genres.count()\n",
    "genres.take(30)\n",
    "print(\"No. of Genres: \" , genreCount , \"List of Genres:\" , genres.take(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduce\">\n",
    "<img align=left src=\"files/images/pyspark-page23.svg\" width=500 height=500 />\n",
    "</a>\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question:</b> Get the average of all of the ratings. There are different solutions:\n",
    "<ul>\n",
    "<li>  use the <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.mean\">`mean`</a> built-in function</li>\n",
    "<li> use the <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduce\">`reduce`</a> function and define you own function for summing two ratings</li>\n",
    "</ul>\n",
    "Compare these approaches in terms of execution time by using `iPython`'s magic command <a href=\"https://ipython.org/ipython-doc/3/interactive/magics.html#magic-timeit\">`timeit`</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%timeit -c\n",
    "averageRating= ratingsRDD.map(lambda x: x[2]).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.581564453029317\n"
     ]
    }
   ],
   "source": [
    "avgRating= ratingsRDD.map(lambda x: (x[2],1)).reduce(lambda accum, n: (accum[0] + n[0], accum[1] + n[1]))\n",
    "print(avgRating[0]/avgRating[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question:</b> Get the average rating for each movie and user.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 4.188679245283019),\n",
       " (2, 3.7131782945736433),\n",
       " (3, 3.9019607843137254),\n",
       " (4, 4.190476190476191),\n",
       " (5, 3.1464646464646466),\n",
       " (6, 3.9014084507042255),\n",
       " (7, 4.32258064516129),\n",
       " (8, 3.884892086330935),\n",
       " (9, 3.7358490566037736),\n",
       " (10, 4.114713216957606)]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aTuple = (0,0)\n",
    "userRatingRDD=ratingsRDD.map(lambda x: (x[0] , x[2])).aggregateByKey(aTuple, lambda a,b: (a[0] + b,    a[1] + 1),\n",
    "                                       lambda a,b: (a[0] + b[0], a[1] + b[1]))\n",
    "\n",
    "userRatingRDD=userRatingRDD.mapValues(lambda v: v[0]/v[1]).sortByKey()\n",
    "\n",
    "userRatingRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 4.146846413095811),\n",
       " (2, 3.20114122681883),\n",
       " (3, 3.01673640167364),\n",
       " (4, 2.7294117647058824),\n",
       " (5, 3.0067567567567566),\n",
       " (6, 3.8787234042553194),\n",
       " (7, 3.410480349344978),\n",
       " (8, 3.014705882352941),\n",
       " (9, 2.656862745098039),\n",
       " (10, 3.5405405405405403)]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cTuple = (0,0)\n",
    "moviesRatingRDD=ratingsRDD.map(lambda x: (x[1] , x[2])).sortByKey()\n",
    "\n",
    "moviesRatingRDD=ratingsRDD.map(lambda x: (x[1] , x[2])).aggregateByKey(cTuple, lambda a,b: (a[0] + b,    a[1] + 1),\n",
    "                                       lambda a,b: (a[0] + b[0], a[1] + b[1]))\n",
    "\n",
    "moviesRatingRDD=moviesRatingRDD.mapValues(lambda v: v[0]/v[1]).sortByKey()\n",
    "\n",
    "moviesRatingRDD.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question:</b> \n",
    "<ul>\n",
    "<li> Get top-$n$ movies with highest average ratings</li>\n",
    "<li> Get top-$n$ Movies with highest average ratings and more than 500 reviews</li>\n",
    "</ul>\n",
    "Save the results on Disk\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(787, 5.0), (989, 5.0), (1830, 5.0), (3172, 5.0), (3233, 5.0)]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topMoviesRDD=moviesRatingRDD.sortBy(lambda x: x[1], False)\n",
    "topMoviesRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question:</b> Compute the sparsity of the rating matrix.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question:</b> Get the rating distribution and plot histogram.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"LibSVM\"> 4) LibSVM Format</a>  \n",
    "\n",
    "\n",
    "<p style=\"text-align: right; font-size: 10px;\"><a href=\"#top\">Go to top</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div class=\"exo\"> <b>Question:</b> Examine the output of MLUtils's <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.util.MLUtils.loadLibSVMFile\">`loadLibSVMFile`</a> routine on the supervised classification datasets below.\n",
    "</div>\n",
    "\n",
    "\n",
    "The elements of the produced RDD have the form of <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LabeledPoint\">`LabeledPoints`</a> composed of a label `example.label` corresponding to the class (+1 or -1) and a feature vector `example.features` generally encoded as a <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.SparseVector\">`SparseVector`</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path to ionosphere LibSVM\n",
    "LibSVMHomeDir=\"data/LibSVM/\"\n",
    "LibName=\"ionosphere.txt\"\n",
    "#LibName=\"rcv1_train.binary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "data = MLUtils.loadLibSVMFile(sc, LibSVMHomeDir + LibName).setName(\"LibSVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question:</b> Count the the number of examples, the number of features, and the sparsity.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question:</b> Create your own LibSVM Reader file (you can use the number of features to simplify writing)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
